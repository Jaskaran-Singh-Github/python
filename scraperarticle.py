# -*- coding: utf-8 -*-
"""ScraperArticle.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1A4FQVhoZ6N7q-bRD6RIdxCmWY-ishk9S
"""

pip install beautifulsoup4

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random

# Constants
BASE_URL = "https://indianexpress.com/"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
}

# Fetch page and parse it
def fetch_page(url):
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        response.raise_for_status()
        return BeautifulSoup(response.text, "html.parser")
    except requests.RequestException as e:
        print(f"Error fetching {url}: {e}")
        return None

# Collect up to 30 unique article links from multiple sections
def extract_article_links(soup, max_links=30):
    links = set()
    for tag in soup.find_all("a", href=True):
        href = tag["href"]
        if (
            href.startswith("https://indianexpress.com/article")
            and href not in links
        ):
            links.add(href)
        if len(links) >= max_links:
            break
    return list(links)

# Extract article title and content
def extract_article_content(url):
    soup = fetch_page(url)
    if soup is None:
        return None, None

    try:
        title = soup.find("h1").get_text(strip=True)
        paragraphs = soup.find_all("p")
        content = "\n".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))
        return title, content
    except Exception as e:
        print(f"Error parsing article {url}: {e}")
        return None, None

# Scraper logic
def scrape_indian_express(max_articles=30):
    homepage_soup = fetch_page(BASE_URL)
    if homepage_soup is None:
        print("Failed to load homepage.")
        return pd.DataFrame()

    article_urls = extract_article_links(homepage_soup, max_articles)
    print(f"Collected {len(article_urls)} article links.")

    data = []

    for idx, url in enumerate(article_urls):
        print(f"[{idx+1}/{len(article_urls)}] Scraping: {url}")
        title, content = extract_article_content(url)
        if title and content:
            data.append({
                "title": title,
                "url": url,
                "content": content
            })

        time.sleep(random.uniform(2, 3.5))

    return pd.DataFrame(data)
if __name__ == "__main__":
    df = scrape_indian_express(max_articles=30)
    if not df.empty:
        df.to_csv("indian_express_30_articles.csv", index=False)
        print(f"Saved {len(df)} articles to indian_express_30_articles.csv")
    else:
        print("No articles scraped.")